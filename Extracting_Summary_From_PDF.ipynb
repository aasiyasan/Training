{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "KQ7HFDtFAHe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai\n",
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "dz9Hx6I8AIqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "icArilnHAMqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "69jbueJLAQsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "id": "NEaJN4fG344D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNyEvzJ9-4zf"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "JbJC9yc4H7OS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3415ddf7-ab9a-41ca-d10f-4740ca239b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHROMA_PATH = \"Chroma\"\n",
        "# ----- Data Indexing Process -----\n",
        "# load your pdf docs\n",
        "DOC_PATH = \"/content/Test.pdf\"\n",
        "# load your pdf doc\n",
        "loader = PyPDFLoader(DOC_PATH)\n",
        "pages = loader.load()\n",
        "# split the doc into smaller chunks i.e. chunk_size=500\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "# get OpenAI Embedding model\n",
        "#embeddings = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1\", model_kwargs={'device': 'cpu'})\n",
        "embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# embed the chunks as vectors and load them into the database.\n",
        "db_chroma = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)\n",
        "# ----- Retrieval and Generation Process -----\n",
        "\n",
        "# this is an example of a user question (query)\n",
        "#query = 'what are the top risks mentioned in the document?'\n",
        "query='Summarize Nike Fiscal 2025 results'"
      ],
      "metadata": {
        "id": "VgQodDdt_LRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve context - top 5 most relevant (closests) chunks to the query vector\n",
        "# (by default Langchain is using cosine distance metric)\n",
        "docs_chroma = db_chroma.similarity_search_with_score(query, k=5)\n",
        "\n",
        "# generate an answer based on given user query and retrieved context information\n",
        "context_text = \"\\n\\n\".join([doc.page_content for doc, _score in docs_chroma])\n",
        "# you can use a prompt template\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "{context}\n",
        "Answer the question based on the above context: {question}.\n",
        "Provide a detailed answer.\n",
        "Don’t justify your answers.\n",
        "Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
        "Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Yc1qYBNB_gcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load retrieved context and user query in the prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "prompt = prompt_template.format(context=context_text, question=query)\n",
        "\n",
        "# call LLM model to generate the answer based on the given context and query\n",
        "#using google genai instead of open AI here\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\" ,api_key=userdata.get('GoogleAPIKey'))\n",
        "\n",
        "response_text = llm.predict(prompt)\n",
        "print(response_text)"
      ],
      "metadata": {
        "id": "AwnjLahY_T6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f87520a-70ba-45a4-be40-b1b4f4ebafb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NIKE, Inc. reported financial results for its fiscal 2025 fourth quarter and full year ended May 31, 2025.\n",
            "\n",
            "For the full fiscal year 2025:\n",
            "*   Full year revenues were $46.3 billion, which was down 10 percent on a reported basis.\n",
            "*   Net income was $0.2 billion, a decrease of 86 percent.\n",
            "*   Diluted earnings per share was $0.14, also an 86 percent decrease.\n",
            "*   Revenues for NIKE, Inc. specifically were $46.3 billion, down 10 percent on a reported basis and down 9 percent on a currency-neutral basis.\n",
            "*   Revenues for the NIKE Brand were $44.7 billion, down 9 percent on a reported and currency-neutral basis, driven by declines across all geographies.\n"
          ]
        }
      ]
    }
  ]
}